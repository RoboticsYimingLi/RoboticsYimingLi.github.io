<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>

    <meta name=viewport content=“width=800”>
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* latin-ext */
        @font-face {
            font-family: 'Lato';
            font-style: italic;
            font-weight: 400;
            src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAUi-qNiXg7eU0.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Lato';
            font-style: italic;
            font-weight: 400;
            src: local('Lato Italic'), local('Lato-Italic'), url(https://fonts.gstatic.com/s/lato/v15/S6u8w4BMUTPHjxsAXC-qNiXg7Q.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Lato';
            font-style: italic;
            font-weight: 700;
            src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_FQftx9897sxZ.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Lato';
            font-style: italic;
            font-weight: 700;
            src: local('Lato Bold Italic'), local('Lato-BoldItalic'), url(https://fonts.gstatic.com/s/lato/v15/S6u_w4BMUTPHjxsI5wq_Gwftx9897g.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjxAwXiWtFCfQ7A.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 400;
            src: local('Lato Regular'), local('Lato-Regular'), url(https://fonts.gstatic.com/s/lato/v15/S6uyw4BMUTPHjx4wXiWtFCc.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        /* latin-ext */
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 700;
            src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwaPGQ3q5d0N7w.woff2) format('woff2');
            unicode-range: U+0100-024F, U+0259, U+1E00-1EFF, U+2020, U+20A0-20AB, U+20AD-20CF, U+2113, U+2C60-2C7F, U+A720-A7FF;
        }

        /* latin */
        @font-face {
            font-family: 'Lato';
            font-style: normal;
            font-weight: 700;
            src: local('Lato Bold'), local('Lato-Bold'), url(https://fonts.gstatic.com/s/lato/v15/S6u9w4BMUTPHh6UVSwiPGQ3q5d0.woff2) format('woff2');
            unicode-range: U+0000-00FF, U+0131, U+0152-0153, U+02BB-02BC, U+02C6, U+02DA, U+02DC, U+2000-206F, U+2074, U+20AC, U+2122, U+2191, U+2193, U+2212, U+2215, U+FEFF, U+FFFD;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }

        body,
        td,
        th,
        tr,
        p,
        a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px
        }

        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 22px;
        }

        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 14px;
            font-weight: 700
        }

        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }

        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }

        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }

        span.highlight {
            background-color: #ffffd0;
        }
    </style>
    <link rel="icon" type="image/jpg" href="img/headshot.jpg">
    <title>Daniel Lenton</title>
    <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet'
          type='text/css'>

</head>
<body>
<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
        <td>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="67%" valign="middle">
                        <p align="center">
                            <name>Yiming LI</name>
                        </p>
                        <p>
                            I'm a first-year PhD student in <a href="https://ai4ce.github.io/">AI4CE Lab</a> at <a href="https://www.nyu.edu/">New York University (NYU)</a> with Dean's PhD Fellowship.
                            In my research, I am exploring the integration of robotic perception, navigation, planning and learning.
                        </p>
                        <p>
                            Prior to NYU, I received my bachelor degree in mechanical engineering from <a href="https://en.tongji.edu.cn/">Tongji University (TJU)</a> with honors, where I have investigated robot localization and low-level visual recognition like visual tracking.
                        </p>
                        <p align=center>
                            <a href="mailto:yimingli@nyu.edu">Email</a> &nbsp/&nbsp
                            <a href="https://github.com/RoboticsYimingLi">Github</a> &nbsp/&nbsp
                            <!--<a href="pdf/cv.pdf">CV</a> &nbsp/&nbsp-->
                            <a href="https://scholar.google.com/citations?hl=en&user=i_aajNoAAAAJ&view_op=list_works">Google Scholar</a>
                            &nbsp/&nbsp
                            <a href="https://www.linkedin.com/in/yiming-li-58b519173/">Linkedin</a>
                        </p>
                    </td>
                    <td width="33%">
                        <a href="img/headshot.jpeg">
                            <img src="img/headshot.png" width="250px"></a>
                    </td>
                </tr>
            </table>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="100%" valign="middle">
                        <heading>Education</heading>
                    </td>
                </tr>
            </table>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="10%">
                        <div class="one">
                            <div class="two"><img src='img/nyu.png' width="160"></div>
                        </div>
                    </td>
                    <td valign="top" width="90%">
                        <strong>Sept. 2020 - Jun. 2025 (Expected) </strong>
                        <br>
                        <strong>PhD Student in Robotics, Dean's PhD Fellowship</strong>
                        <br>
                        Automation and Intelligence for Civil Engineering (AI4CE) Group
                        <br>
                        Tandon School of Engineering, New York University
                        <br>


<!--                        Work with-->
<!--                        <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ&hl=en">Prof. Chen Feng</a>-->
                        <p></p>
                        <p>On advancing fundamental automation and intelligence technologies, and addressing challenges of their applications in civil and mechanical engineering.</p>
                    </td>
                </tr>
                <tr>
                    <td width="10%">
                        <div class="one">
                            <div class="two"><img src='img/TJU.png' width="150"></div>
                        </div>
                    </td>
                    <td valign="top" width="90%">
                        <strong>Sept. 2015 - Jun. 2020 </strong>
                        <br>
                        <strong>Bachelor of Engineering in Mechanical Engineering</strong>
                        <br>
                        Tongji University, Shanghai
                        <br>
                        Undergraduate Academic Star (top 10 students from 18,115 undergraduates)
                        <br>
                        Shanghai Outstanding Graduate, 1/114
<!--                        <br>-->
<!--                        Work with-->
<!--                        <a href="https://scholar.google.com/citations?user=zmbMZ4kAAAAJ&hl=en">Prof. Changhong Fu</a>-->
                        <p></p>
                        <p>National Scholarship, Shanghai Scholarship, Outstanding Undergraduate Thesis, Model of Outstanding Students, etc.</p>
                    </td>
                </tr>
            </table>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="100%" valign="middle">
                        <heading>Research and Teaching Experience</heading>
                    </td>
                </tr>
            </table>
           <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="10%">
                        <div class="one">
                            <div class="two"><img src='img/nyu.png' width="145"></div>
                        </div>
                    </td>
                    <td valign="top" width="90%">
                        <strong>Jun. 2020 - Present </strong>
                        <br>
                        <strong>Research Assistant, New York University, NYC</strong>
                        <br>
                        Tandon School of Engineering
                        <br>
                        AI4CE Group
                        <br>
                        Work with
                        <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ&hl=en">Prof. Chen Feng</a>
                        <p></p>
                        <p>Conduct research in robot perception, navigation and planning.</p>
                    </td>
                </tr>

                <tr>
                    <td width="10%">
                        <div class="one">
                            <div class="two"><img src='img/SJTU.png' width="140"></div>
                        </div>
                    </td>
                    <td valign="top" width="90%">
                        <strong>Mar. 2021 - Jun. 2021 </strong>
                        <br>
                        <strong>Research Intern, Shanghai Jiao Tong University, Shanghai</strong>
                        <br>
                        School of Electronic Information and Electrical Engineering
                        <br>
                        Cooperative Medianet Innovation Center (CMIC)
                        <br>
                        Work with
                        <a href="https://scholar.google.com/citations?user=W_Q33RMAAAAJ&hl=zh-CN">Prof. Siheng Chen</a>
                        <p></p>
                        <p>Conduct research in cooperative perception, multi-agent communication and graph theory.</p>
                    </td>
                </tr>
               <tr>
                    <td width="10%">
                        <div class="one">
                            <div class="two"><img src='img/THU.png' width="140"></div>
                        </div>
                    </td>
                    <td valign="top" width="90%">
                        <strong>Dec. 2020 - Jan. 2021 </strong>
                        <br>
                        <strong>Visiting PhD Student, Tsinghua University, Beijing</strong>
                        <br>
                        Institute for Interdisciplinary Information Sciences (IIIS)
                        <br>
                        MARS Lab
                        <br>
                        Work with
                        <a href="https://scholar.google.com/citations?user=DmahiOYAAAAJ&hl=zh-CN">Prof. Hang Zhao</a>
                        <p></p>
                        <p>Conduct research in motion prediction and latency-aware predictive perception.</p>
                    </td>
                </tr>
               <tr>
                    <td width="10%">
                        <div class="one">
                            <div class="two"><img src='img/nyush.png' width="140"></div>
                        </div>
                    </td>
                    <td valign="top" width="90%">
                        <strong>Sept. 2020 - Dec. 2021 </strong>
                        <br>
                        <strong>Teaching Assistant, NYUSH, Shanghai</strong>
                        <br>
                        Department of Mathematics
                        <br>
                        Teaching Go Local Students
                        <br>
<!--                        Work with-->
<!--                        <a href="https://scholar.google.com/citations?user=DmahiOYAAAAJ&hl=zh-CN">Prof. Hang Zhao</a>-->
                        <p></p>
                        <br>MATH-UA 121-046: Calculus 1, Fall 2020
                        <br>MATH-UA 211 Mathematics for Economics I, Fall 2020
                        <br>Precalculus, Fall 2020
                    </td>
                </tr>
               <tr>
                    <td width="10%">
                        <div class="one">
                            <div class="two"><img src='img/TJU.png' width="145"></div>
                        </div>
                    </td>
                    <td valign="top" width="90%">
                        <strong>Jul. 2018 - Jun. 2020 </strong>
                        <br>
                        <strong>Research Assistant, Tongji University, Shanghai</strong>
                        <br>
                        School of Mechanical Engineering
                        <br>
                        Vision4Robotics Group
                        <br>
                        Work with
                        <a href="https://scholar.google.com/citations?user=zmbMZ4kAAAAJ&hl=en">Prof. Changhong Fu</a>
                        <p></p>
                        <p>Conduct research in visual object tracking in the air.</p>
                    </td>
                </tr>
               <tr>
                    <td width="10%">
                        <div class="one">
                            <div class="two"><img src='img/THU.png' width="140"></div>
                        </div>
                    </td>
                    <td valign="top" width="90%">
                        <strong>Oct. 2019 - Nov. 2019 </strong>
                        <br>
                        <strong>Research Assistant, Tsinghua University, Beijing</strong>
                        <br>
                        Department of Automation
                        <br>
                        UAV Lab
                        <br>
                        Work with
                        <a href="https://www.researchgate.net/profile/Geng-Lu-3">Prof. Geng Lu</a>
                        <p></p>
                        <p>Conduct research in the localization and navigation for aerial robots.</p>
                    </td>
                </tr>
            </table>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td width="100%" valign="middle">
                        <heading>Research and Publications</heading>
                    </td>
                </tr>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

                <tr>
                    <td width="25%">
                        <div class="one">
                            <div class="two"><img src='img/FLAT.png' width=170></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <a href="https://djl11.github.io/ESM/">
                            <papertitle>Fooling LiDAR Perception via Adversarial Trajectory Perturbation</papertitle>
                        </a>
                        <br>
                        <strong>Yiming Li</strong>,
                        <a href="https://scholar.google.com/citations?hl=en&user=OTBgvCYAAAAJ">Congcong Wen</a>,
                        <a href="https://scholar.google.com/citations?hl=en&user=dgN8vtwAAAAJ">Felix Juefei-Xu</a>,
                        <a href="https://scholar.google.com/citations?user=YeG8ZM0AAAAJ&hl=en">Chen Feng</a>
                        <br>
                        <em>Arxiv, preprint</em>, 2021
<!--                        <em>International Conference on Learning Representations (<b>ICLR</b>)</em>, 2021-->
                        <br>
                        <a href="">paper</a> /
                        <a href="">video</a> /
                        <a href="">code</a> /
                        <a href="https://djl11.github.io/ESM/">project page</a>
                        <p></p>
                        <p>LiDAR point clouds collected from a vehicle are functions of its trajectories, because the sensor motion needs to be compensated to avoid distortions.
When autonomous vehicles are sending LiDAR point clouds to deep networks for perception and planning, could the motion compensation consequently become a wide-open backdoor in those networks? </p>
                    </td>
                </tr>

                <tr>
                    <td width="25%">
                        <div class="one">
                            <div class="two"><img src='img/PVT.png' width="170"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <a href="http://people.csail.mit.edu/hangzhao/PVT/">
                            <papertitle>Predictive Visual Tracking: A New Benchmark and Baseline Approach</papertitle>
                        </a>
                        <br>
                        <a href="https://scholar.google.com/citations?user=XjZjcakAAAAJ&hl=en">Bowen Li*</a>,
                        <strong>Yiming Li*</strong>,
                        <a href="https://scholar.google.com/citations?user=uy-TfXgAAAAJ&hl=en">Junjie Ye</a>,
                        <a href="https://scholar.google.com/citations?user=zmbMZ4kAAAAJ&hl=en">Changhong Fu</a>,
                        <a href="https://scholar.google.com/citations?user=DmahiOYAAAAJ&hl=en">Hang Zhao</a>
                        <br>
                        <em>Arxiv, preprint</em>, 2021
<!--                        <em>International Conference on Learning Representations (<b>ICLR</b>)</em>, 2021-->
                        <br>
                        <a href="https://arxiv.org/pdf/2103.04508">paper</a> /
                        <a href="https://www.youtube.com/watch?v=n8i8bREIFeM">video</a> /
                        <a href="https://github.com/vision4robotics/LAE-PVT-master">code</a> /
                        <a href="http://people.csail.mit.edu/hangzhao/PVT/">project page</a>
                        <p></p>
                        <p> In this work, we deal with a more realistic problem of latency-aware tracking. The state-of-the-art trackers are evaluated in the aerial scenarios with new metrics jointly assessing the tracking accuracy and efficiency. Moreover, a new predictive visual tracking baseline is developed to compensate for the latency stemming from the onboard computation. </p>
                    </td>
                </tr>

                <tr>
                    <td width="25%">
                        <div class="one">
                            <div class="two"><img src='img/AutoTrack.png' width="170"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_AutoTrack_Towards_High-Performance_Visual_Tracking_for_UAV_With_Automatic_Spatio-Temporal_CVPR_2020_paper.pdf">
                            <papertitle>AutoTrack: Towards High-Performance Visual Tracking for UAV with Automatic Spatio-Temporal Regularization</papertitle>
                        </a>
                        <br>
                        <strong>Yiming Li</strong>,
                        <a href="https://scholar.google.com/citations?user=zmbMZ4kAAAAJ&hl=en">Changhong Fu</a>,
                        <a href="https://scholar.google.com/citations?user=Ja8dgh8AAAAJ&hl=en">Fangqiang Ding</a>,
                        <a href="https://scholar.google.com/citations?user=A9D-disAAAAJ&hl=en">Ziyuan Huang</a>,
                        <a href="https://www.researchgate.net/profile/Geng-Lu-3">Geng Lu</a>
                        <br>
                        <em>IEEE Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>)</em>, 2020
                        <br>
                        <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Li_AutoTrack_Towards_High-Performance_Visual_Tracking_for_UAV_With_Automatic_Spatio-Temporal_CVPR_2020_paper.pdf">paper</a> /
<!--                        <a href="https://www.youtube.com/watch?v=AympTZtQAE8&t=2s">video</a> /-->
                        <a href="https://github.com/vision4robotics/AutoTrack">code</a>
<!--                        <a href="https://ivy-dl.org/">project page</a>-->
                        <p></p>
                        <p>Predefined parameters introduce much effort in tuning them and they still fail to adapt to new situations that the designer did not think of. In this work, a novel approach is proposed to online automatically and adaptively learn spatio-temporal regularization term.</p>
                    </td>
                </tr>

                <tr>
                    <td width="25%">
                        <div class="one">
                            <div class="two"><img src='img/KAOT.png' width="170"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <a href="https://arxiv.org/pdf/2003.05218">
                            <papertitle>Keyfilter-Aware Real-Time UAV Object Tracking</papertitle>
                        </a>
                        <br>
                        <strong>Yiming Li</strong>,
                        <a href="https://scholar.google.com/citations?user=zmbMZ4kAAAAJ&hl=en">Changhong Fu</a>,
                        <a href="https://scholar.google.com/citations?user=A9D-disAAAAJ&hl=en">Ziyuan Huang</a>,
                        <a href="https://scholar.google.com/citations?user=qxNq7PQAAAAJ&hl=en">Yinqiang Zhang</a>,
                        <a href="https://scholar.google.com/citations?user=YYT8-7kAAAAJ&hl=en">Jia Pan</a>
                        <br>
                        <em>IEEE International Conference on Robotics and Automation (<b>ICRA</b>)</em>, 2020
                        <br>
                        <a href="https://arxiv.org/pdf/2003.05218">paper</a> /
                        <a href="https://www.youtube.com/watch?v=jMfmHVRqv3Y">video</a> /
                        <a href="https://github.com/vision4robotics/KAOT-tracker">code</a>
                        <p></p>
                        <p>Inspired by keyframe-based SLAM, keyfilter-aware visual object tracking is proposed: keyfilters generated by periodically selected keyframes learn the context intermittently and are used to restrain the learning of filters.</p>
                    </td>
                </tr>

                <tr>
                    <td width="25%">
                        <div class="one">
                            <div class="two"><img src='img/AMCF.png' width="170"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <a href="https://arxiv.org/pdf/1909.10989">
                            <papertitle>Augmented Memory for Correlation Filters in Real-Time UAV Tracking</papertitle>
                        </a>
                        <br>
                        <strong>Yiming Li</strong>,
                        <a href="https://scholar.google.com/citations?user=zmbMZ4kAAAAJ&hl=en">Changhong Fu</a>,
                        <a href="https://scholar.google.com/citations?user=Ja8dgh8AAAAJ&hl=en">Fangqiang Ding</a>,
                        <a href="https://scholar.google.com/citations?user=A9D-disAAAAJ&hl=en">Ziyuan Huang</a>,
                        <a href="https://scholar.google.com/citations?user=YYT8-7kAAAAJ&hl=en">Jia Pan</a>
                        <br>
                        <em>IEEE/RSJ International Conference on Intelligent Robots and Systems (<b>IROS</b>)</em>, 2020
                        <br>
                        <a href="https://arxiv.org/pdf/1909.10989.pdf">paper</a> /
                        <a href=" https://www.youtube.com/watch?v=CGH5o2J1ohI">video</a> /
                        <a href=" https://github.com/vision4robotics/AMCF-tracker">code</a>
                        <p></p>
                        <p>In this work, a novel tracker based on DCF framework is proposed to augment memory of previously appeared views while running at real-time speed. Several historical views and the current view are simultaneously introduced in training to allow the tracker to adapt to new appearances as well as memorize previous ones.</p>
                    </td>
                </tr>
                <tr>
                    <td width="25%">
                        <div class="one">
                            <div class="two"><img src='img/ARCF.png' width="170"></div>
                        </div>
                    </td>
                    <td valign="top" width="75%">
                        <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Learning_Aberrance_Repressed_Correlation_Filters_for_Real-Time_UAV_Tracking_ICCV_2019_paper.pdf">
                            <papertitle>Learning Aberrance Repressed Correlation Filters for Real-Time UAV Tracking</papertitle>
                        </a>
                        <br>
                        <a href="https://scholar.google.com/citations?user=A9D-disAAAAJ&hl=en">Ziyuan Huang</a>,
                        <a href="https://scholar.google.com/citations?user=zmbMZ4kAAAAJ&hl=en">Changhong Fu</a>,
                        <strong>Yiming Li</strong>,
                        <a href="https://vision4robotics.github.io/authors/fuling-lin/">Fuling Lin</a>,

                        <a href="https://scholar.google.com/citations?user=ts7ItWgAAAAJ&hl=en">Peng Lu</a>
                        <br>
                        <em>IEEE International Conference on Computer Vision (<b>ICCV</b>)</em>, 2019
                        <br>
                        <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Huang_Learning_Aberrance_Repressed_Correlation_Filters_for_Real-Time_UAV_Tracking_ICCV_2019_paper.pdf">paper</a> /
<!--                        <a href="https://www.youtube.com/watch?v=AympTZtQAE8&t=2s">video</a> /-->
                        <a href="https://github.com/vision4robotics/ARCF-tracker">code</a>
<!--                        <a href="https://ivy-dl.org/">project page</a>-->
                        <p></p>
                        <p>In this work, a novel approach to repress the aberrances happening during the detection process is proposed, i.e., aberrance repressed correlation filter (ARCF). By enforcing restriction to the rate of alteration in response maps generated in the detection phase, the ARCF tracker can evidently suppress aberrances and is thus more robust and accurate to track objects.</p>
                    </td>
                </tr>
            </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td>
                        <br>
                        <p align="right">
                            <font size="2">
                                Website template by <a href="https://jonbarron.info">Jon Barron</a>.
                            </font>
                        </p>
                    </td>
                </tr>
            </table>

        </td>
    </tr>
</table>
</body>
</html>
